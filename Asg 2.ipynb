{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import nltk\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem.snowball import EnglishStemmer \n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import dmba\n",
    "from dmba import printTermDocumentMatrix, classificationSummary, liftChart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\oscar\\Downloads\\farm-ads.csv\")\n",
    "\n",
    "# Assign column names\n",
    "df.columns = ['Label', 'Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Label'] = df['Label'].map({-1: 0, 1: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ad-abdominal ad-aortic ad-aneurysm ad-doctorf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>ad-abdominal ad-aortic ad-aneurysm ad-million...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>ad-absorbent ad-oil ad-snar ad-factory ad-dir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>ad-acid ad-reflux ad-relief ad-top ad-treatme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>ad-acid ad-reflux ad-symptom ad-acid ad-reflu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label                                               Text\n",
       "0      0   ad-abdominal ad-aortic ad-aneurysm ad-doctorf...\n",
       "1      0   ad-abdominal ad-aortic ad-aneurysm ad-million...\n",
       "2      0   ad-absorbent ad-oil ad-snar ad-factory ad-dir...\n",
       "3      0   ad-acid ad-reflux ad-relief ad-top ad-treatme...\n",
       "4      0   ad-acid ad-reflux ad-symptom ad-acid ad-reflu..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn features based on text\n",
    "count_vect = CountVectorizer()\n",
    "counts = count_vect.fit_transform(df['Text'])\n",
    "\n",
    "# Apply TF-IDF Normalization\n",
    "transformer = TfidfTransformer()\n",
    "X_tfidf = transformer.fit_transform(counts)\n",
    "\n",
    "# Dimensionality Reduction to create a Concept-Document Matrix\n",
    "svd_model = TruncatedSVD(n_components=20, random_state=42)\n",
    "X_concept = svd_model.fit_transform(X_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequently appearing terms in the corpus:\n",
      "ad         45437\n",
      "title      23542\n",
      "header     14509\n",
      "list       11228\n",
      "product    11154\n",
      "com        10704\n",
      "pet         7057\n",
      "health      6202\n",
      "home        5870\n",
      "free        5854\n",
      "dtype: int64\n",
      "Entries for 'pet':\n",
      "0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "4138    53\n",
      "4139    53\n",
      "4140    53\n",
      "4141    49\n",
      "4142    53\n",
      "Name: pet, Length: 4143, dtype: int64\n",
      "Entries for 'health':\n",
      "0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        1\n",
      "        ..\n",
      "4138    10\n",
      "4139     9\n",
      "4140     9\n",
      "4141     9\n",
      "4142     9\n",
      "Name: health, Length: 4143, dtype: int64\n",
      "Total count for 'pet': 7057\n",
      "Total count for 'health': 6202\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'counts' is the Term-Document matrix you have from the CountVectorizer\n",
    "# Convert it to a dense format because the TDM is typically sparse\n",
    "dense_counts = counts.toarray()\n",
    "\n",
    "# Convert to DataFrame for easier handling\n",
    "terms = count_vect.get_feature_names_out()  # This gets the list of terms in the order they're stored in the matrix\n",
    "tdm_df = pd.DataFrame(dense_counts, columns=terms)\n",
    "\n",
    "# Sum up each term's occurrences across all documents and sort them to find the most frequent terms\n",
    "most_frequent_terms = tdm_df.sum(axis=0).sort_values(ascending=False).head(10)  # Adjust as necessary to see more terms\n",
    "\n",
    "# Print the most frequent terms\n",
    "print(\"Most frequently appearing terms in the corpus:\")\n",
    "print(most_frequent_terms)\n",
    "\n",
    "# Print the entries for 'pet' and 'health'\n",
    "print(f\"Entries for 'pet':\\n{tdm_df['pet']}\")\n",
    "print(f\"Entries for 'health':\\n{tdm_df['health']}\")\n",
    "\n",
    "# Print the total counts for 'pet' and 'health'\n",
    "print(f\"Total count for 'pet': {tdm_df['pet'].sum()}\")\n",
    "print(f\"Total count for 'health': {tdm_df['health'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chosen 2 entries are pet and health.</br>\n",
    "The first non-zero entry is 'pet', indicates that the term 'pet' appears 53 time in the 4138th document. The second non-zero entry is 'health', indicates that the term 'health' appears 10 time in the 4138th document. It may show that pet health is the main focus of ad 4138.</br>\n",
    "By looking at the frequncy table, pet and health are both top 10 most frequent appeared terms in the dataset, which means that the ad in the dataset are mainly related to pet and health</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\oscar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.stemmer = EnglishStemmer()\n",
    "        self.stopWords = set(ENGLISH_STOP_WORDS)\n",
    "    def __call__(self, doc):\n",
    "        return [self.stemmer.stem(t) for t in word_tokenize(doc) \n",
    "                if t.isalpha() and t not in self.stopWords]\n",
    "\n",
    "preprocessor = CountVectorizer(tokenizer=LemmaTokenizer(), encoding='latin1')\n",
    "preprocessedText = preprocessor.fit_transform(df['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: TF-IDF and latent semantic analysis\n",
    "tfidfTransformer = TfidfTransformer()\n",
    "tfidf = tfidfTransformer.fit_transform(preprocessedText)\n",
    "\n",
    "# Extract 20 concepts using LSA ()\n",
    "svd = TruncatedSVD(20)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "lsa_tfidf = lsa.fit_transform(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Accuracy 0.7992)\n",
      "\n",
      "       Prediction\n",
      "Actual   0   1\n",
      "     0 346 158\n",
      "     1  50 482\n"
     ]
    }
   ],
   "source": [
    "label = df['Label']\n",
    "\n",
    "# split dataset into 60% training and 40% test set\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(lsa_tfidf, label, test_size=0.25, random_state=42)\n",
    "\n",
    "# run logistic regression model on training\n",
    "logit_reg = LogisticRegression(solver='lbfgs')\n",
    "logit_reg.fit(Xtrain, ytrain)\n",
    "\n",
    "# print confusion matrix and accuracty\n",
    "classificationSummary(ytest, logit_reg.predict(Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.85530547 0.88906752 0.87439614 0.88083736 0.86634461]\n",
      "Mean CV accuracy: 0.8731902180385335\n",
      "Accuracy Score on the test set: 0.859073359073359\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming 'X_concept' is your Concept-Document Matrix and 'label' contains your target labels\n",
    "\n",
    "# Split dataset into 75% training and 25% test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_concept, label, test_size=0.25, random_state=42)\n",
    "\n",
    "# Run logistic regression model on training set\n",
    "logit_reg = LogisticRegression(solver='lbfgs', max_iter=1000)  # Increased max_iter for convergence if needed\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(logit_reg, X_train, y_train, cv=5)\n",
    "\n",
    "logit_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = logit_reg.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print the accuracy score and the cross-validation scores\n",
    "print(f'Cross-validation scores: {cv_scores}')\n",
    "print(f'Mean CV accuracy: {np.mean(cv_scores)}')\n",
    "print(f'Accuracy Score on the test set: {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
